{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 316.9 MB 54 kB/s  eta 0:00:0161\n",
      "\u001b[?25hCollecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[K     |████████████████████████████████| 200 kB 70.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425365 sha256=f08c14a4441853e8e4376c06e4a9172d6889962f0590fed34a9994eac669283b\n",
      "  Stored in directory: /Users/thanhpham/Library/Caches/pip/wheels/57/bd/14/ce9e21f2649298678d011fb8f71ed38ee70b42b94fef0be142\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing the PySpark library\n",
    "# Note: While PySpark is the Python API for Apache Spark, if you're using Azure Databricks, \n",
    "# you don't need to install Spark as it's already integrated. However, if you're setting up \n",
    "# Spark in a different environment, you'll need this step.\n",
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary functions from PySpark's functions module\n",
    "from pyspark.sql.functions import col  # Importing column function for referencing DataFrame columns\n",
    "\n",
    "# Importing specific data types from PySpark's types module\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType\n",
    "\n",
    "# Importing the round function from PySpark's functions module for rounding numerical values\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Using an alias 'F' for the functions module to make it more concise in the code\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Importing the when function from PySpark's functions module for conditional column operations\n",
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7fb164-3f51-4950-9bfa-9e9c0dba1192",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    # Defining the authentication type for Azure Data Lake Storage Gen2 (ADLS Gen2)\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "\n",
    "    # Specifying the provider type for OAuth2 authentication with ADLS Gen2\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "\n",
    "    # Azure application (client) ID associated with the ADLS Gen2 storage account (find in the Azure portal app registration)\n",
    "    \"fs.azure.account.oauth2.client.id\": \"***\",  # Replace with your client ID\n",
    "\n",
    "    # Azure client secret associated with the app registration (should be stored securely, e.g., in Azure Key Vault)\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"***\",  # Replace with your client secret\n",
    "\n",
    "    # OAuth 2.0 token endpoint to fetch the access token for Azure AD authentication\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/***/oauth2/token\"  # Replace with your endpoint\n",
    "}\n",
    "\n",
    "# Mounting an Azure Data Lake Storage Gen2 filesystem to Databricks\n",
    "dbutils.fs.mount(\n",
    "    # The full path to the ADLS Gen2 filesystem. Replace with your specific path.\n",
    "    source = \"abfss://***@***.dfs.core.windows.net\",  # Follow the format: \"abfss://datacontainer@storageaccount.dfs.core.windows.net\"\n",
    "\n",
    "    # The mount point in Databricks File System (DBFS) where the ADLS Gen2 filesystem will be mounted\n",
    "    mount_point = \"/mnt/tokyoolympic\",\n",
    "\n",
    "    # Passing the above-defined configuration settings for the mount operation\n",
    "    extra_configs = configs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5aeba91-4735-4b13-84ba-2f8a316b2e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extract the data into the workspace\n",
    "athletes = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"Inferschema\",\"True\").load(\"/mnt/tokyoolympic/raw-data/athletes.csv\")\n",
    "coaches = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"Inferschema\",\"True\").load(\"/mnt/tokyoolympic/raw-data/coaches.csv\")\n",
    "medals = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"Inferschema\",\"True\").load(\"/mnt/tokyoolympic/raw-data/medals.csv\")\n",
    "teams = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"Inferschema\",\"True\").load(\"/mnt/tokyoolympic/raw-data/teams.csv\")\n",
    "entriesgender = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"Inferschema\",\"True\").load(\"/mnt/tokyoolympic/raw-data/entriesgender.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ce781912-b043-48cf-bb77-97a449c844ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Renaming the column 'Team_Country' to 'Country' in the 'medals' DataFrame\n",
    "medal_rename = medals.withColumnRenamed(\"Team_Country\", \"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4028214-255a-48c7-94e7-9a035c8ef6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correcting country name from \"C�te d'Ivoire\" to \"Ivory Coast\" in the 'athletes' DataFrame\n",
    "athletes_change=athletes.withColumn('Country',when(col('Country')== \"C�te d'Ivoire\", \"Ivory Coast\").otherwise (col('Country')))\n",
    "\n",
    "# Making multiple corrections in the 'teams' DataFrame:\n",
    "# 1. Changing country name from \"C�te d'Ivoire\" to \"Ivory Coast\"\n",
    "# 2. Changing team name from \"C�te d�Ivoire\" to \"Ivory Coast\"\n",
    "# 3. Correcting event names containing garbled characters\n",
    "\n",
    "teams_change= teams.withColumn('Country',when(col('Country')== \"C�te d'Ivoire\", \"Ivory Coast\").otherwise (col('Country'))).\\\n",
    "                    withColumn('TeamName',when(col('TeamName')== \"C�te d�Ivoire\", \"Ivory Coast\").otherwise (col('TeamName'))).\\\n",
    "                    withColumn('Event',when(col('Event')== \"Women's �p�e Team\", \"Women's Épée Team\").otherwise (col('Event'))).\\\n",
    "                    withColumn('Event',when(col('Event')== \"Men's �p�e Team\", \"Men's Épée Team\").otherwise (col('Event')))    \n",
    "                \n",
    "# Correcting country name from \"C�te d'Ivoire\" to \"Ivory Coast\" in the 'coaches' DataFrame\n",
    "coaches_change = coaches.withColumn('Country', when(col('Country') == \"C�te d'Ivoire\", \"Ivory Coast\").otherwise(col('Country')))\n",
    "\n",
    "# Correcting country name from \"C�te d'Ivoire\" to \"Ivory Coast\" in the previously renamed 'medal_rename' DataFrame\n",
    "medals_change = medal_rename.withColumn ('Country',when (col('Country') ==\"C�te d'Ivoire\", \"Ivory Coast\").otherwise(col('Country')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b05e7102-a1c3-4ac1-a4c2-e26b108680c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modifying the 'entriesgender' DataFrame by casting column data types\n",
    "entriesgender = (\n",
    "    entriesgender\n",
    "    # Casting the 'Female' column to Integer type\n",
    "    .withColumn(\"Female\", col(\"Female\").cast(IntegerType()))\n",
    "    # Casting the 'Male' column to Integer type\n",
    "    .withColumn(\"Male\", col(\"Male\").cast(IntegerType()))\n",
    "    # Casting the 'Total' column to Integer type\n",
    "    .withColumn(\"Total\", col(\"Total\").cast(IntegerType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eca0598e-8a5e-466d-97b0-b939d07c961c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grouping the 'athletes_change' DataFrame by 'Country' to count the number of athletes representation per country\n",
    "athletes_total = (\n",
    "    athletes_change\n",
    "    .groupBy(\"Country\")\n",
    "    # Aggregating to count the number of unique athlete names per country and renaming the resulting column\n",
    "    .agg(F.count(\"PersonName\").alias(\"Athletes_Rep\"))\n",
    "    # Sorting the countries by the number of athlete representations in descending order\n",
    "    .orderBy(F.desc(\"Athletes_Rep\"))\n",
    ")\n",
    "\n",
    "# Joining the aggregated athlete representation data with the 'medals_change' DataFrame based on the 'Country' column\n",
    "athletes_medals = athletes_total.join(medals_change, 'Country', 'inner')\n",
    "\n",
    "# Casting the 'Athletes_Rep' column to an Integer type in the resulting 'athletes_medals' DataFrame\n",
    "athletes_medals_dataset = athletes_medals.withColumn(\"Athletes_Rep\", col(\"Athletes_Rep\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d484f13d-e9c5-46fc-a9df-97c32f1fe18c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(athletes_medals_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4105a2f8-28ac-4005-a366-fd370d4b1073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Loading into Data Lake\n",
    "coaches_change.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/coaches')\n",
    "athletes_change.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/athletes')\n",
    "entriesgender.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/entriesgender')\n",
    "medals_change.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/medals')\n",
    "teams_change.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/teams')\n",
    "athletes_medals_dataset.repartition(1).write.mode('overwrite').option('header','true').csv('/mnt/tokyoolympic/transformed-data/athletesmedal')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2509193511393157,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transformation - Tokyo Olympic",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
